{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d763e4f-c3c8-4fa7-abfa-9cadd8194edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q1. What is Lasso Regression, and how does it differ from other regression techniques? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Lasso Regression (Least Absolute Shrinkage and Selection Operator Regression):\n",
    "\n",
    "Lasso Regression is a linear regression technique that combines both prediction and variable selection by adding \n",
    "a penalty to the ordinary least squares (OLS) cost function. The penalty is based on the sum of the absolute values\n",
    "of the coefficients. Lasso Regression is particularly effective in situations where there are many predictors, some\n",
    "of which might be irrelevant or redundant. It encourages some coefficients to become exactly zero, effectively \n",
    "performing automatic feature selection.\n",
    "\n",
    "Key Differences from Other Regression Techniques:\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "Lasso Regression: Lasso can drive some coefficients to become exactly zero, effectively performing feature \n",
    "selection. It's particularly useful when you suspect that only a subset of predictors is relevant.\n",
    "Ridge Regression: While Ridge can shrink coefficients towards zero, it does not eliminate any coefficient entirely.\n",
    "It focuses more on reducing the impact of multicollinearity.\n",
    "\n",
    "Penalty Term:\n",
    "\n",
    "Lasso Regression: The penalty term added to the cost function is based on the sum of the absolute values of \n",
    "coefficients. This leads to a \"L1 regularization\" term that enforces sparsity in the coefficient values.\n",
    "Ridge Regression: The penalty term added in Ridge Regression is based on the sum of squared coefficients, leading \n",
    "to a \"L2 regularization\" term. It prevents coefficients from becoming too large.\n",
    "\n",
    "Coefficient Behavior:\n",
    "\n",
    "Lasso Regression: Lasso tends to shrink coefficients towards zero more aggressively, making it more likely to \n",
    "completely eliminate coefficients, especially when there's multicollinearity.\n",
    "Ridge Regression: Ridge is less likely to drive coefficients exactly to zero. It shrinks coefficients towards \n",
    "zero but does not perform feature elimination as directly as Lasso.\n",
    "\n",
    "Impact on Model Complexity:\n",
    "\n",
    "Lasso and Ridge: Both Lasso and Ridge can help control model complexity by adding penalties to the coefficients.\n",
    "However, Lasso has a stronger impact on reducing model complexity due to its feature elimination behavior.\n",
    "\n",
    "Bias-Variance Trade-off:\n",
    "\n",
    "Lasso and Ridge: Both techniques provide a bias-variance trade-off. By increasing the regularization strength \n",
    "(lambda), you can increase bias and reduce variance, helping to prevent overfitting.\n",
    "\n",
    "Choice of Lambda:\n",
    "\n",
    "Lasso and Ridge: Choosing the optimal lambda (regularization parameter) is critical. Cross-validation and other\n",
    "techniques are used to find the right balance between regularization and model fit.\n",
    "\n",
    "Suitable for High-Dimensional Data:\n",
    "\n",
    "Lasso: Lasso is well-suited for situations with many predictors and limited observations, making it a popular \n",
    "choice in fields like genetics and finance.\n",
    "Ridge: Ridge is also useful for high-dimensional data, especially when multicollinearity is a concern. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f3d6a-f460-4268-b55a-7a4ffa7f6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q2. What is the main advantage of using Lasso Regression in feature selection? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" \n",
    "The main advantage of using Lasso Regression in feature selection is its ability to perform automatic and efficient\n",
    "feature selection by driving some coefficients to exactly zero. This attribute makes Lasso Regression a powerful \n",
    "tool in scenarios where you have a large number of predictors and you suspect that only a subset of them are truly\n",
    "relevant to the outcome. Here are the key advantages of using Lasso Regression for feature selection:\n",
    "\n",
    "Automatic Feature Selection:\n",
    "\n",
    "Lasso Regression inherently selects a subset of the most relevant predictors by forcing some coefficients to become\n",
    "exactly zero.\n",
    "This automatic feature selection process helps to simplify the model and improve its interpretability by focusing \n",
    "only on the most influential predictors.\n",
    "\n",
    "Reduced Model Complexity:\n",
    "\n",
    "By eliminating irrelevant or redundant features, Lasso reduces the complexity of the model, which can lead to \n",
    "improved generalization to new, unseen data.\n",
    "Fewer features mean a simpler model that is less prone to overfitting and easier to understand.\n",
    "\n",
    "Improved Interpretability:\n",
    "\n",
    "A smaller set of predictors with clear relationships to the outcome makes the model more interpretable and easier\n",
    "to communicate to stakeholders.\n",
    "You can focus your analysis on the selected predictors and understand their individual contributions.\n",
    "\n",
    "Dealing with High-Dimensional Data:\n",
    "\n",
    "In situations where the number of predictors is much larger than the number of observations, Lasso is effective \n",
    "in selecting a subset of relevant features without overfitting.\n",
    "Lasso's ability to control model complexity is especially valuable in these high-dimensional settings.\n",
    "\n",
    "Multicollinearity Handling:\n",
    "\n",
    "Lasso is known to handle multicollinearity effectively by selecting one of the correlated predictors while \n",
    "shrinking others towards zero.\n",
    "This helps in identifying the most important variables among correlated ones.\n",
    "\n",
    "Pruning Redundant Predictors:\n",
    "\n",
    "Lasso eliminates redundant predictors, which can improve model performance and reduce the risk of \"curse of\n",
    "dimensionality.\"\n",
    "\n",
    "Preventing Overfitting:\n",
    "\n",
    "The feature selection capability of Lasso helps to prevent overfitting by reducing the number of features used\n",
    "to fit the model.\n",
    "\n",
    "Data-Driven Approach:\n",
    "\n",
    "Instead of relying on domain knowledge or manual selection, Lasso's feature selection is driven by the data \n",
    "itself, making it adaptable to various scenarios. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722c4eb-d7d8-47f4-b7db-1b08f6d10efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q3. How do you interpret the coefficients of a Lasso Regression model? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Interpreting the coefficients of a Lasso Regression model follows similar principles to interpreting \n",
    "coefficients in ordinary least squares (OLS) regression. However, due to Lasso's feature selection behavior and\n",
    "the sparsity it introduces, there are some nuances to consider. Here's how you can interpret the coefficients of\n",
    "a Lasso Regression model:\n",
    "\n",
    "Magnitude of Coefficients:\n",
    "\n",
    "Just like in OLS regression, the sign of a coefficient (positive or negative) indicates the direction of the \n",
    "relationship between the predictor and the response variable.\n",
    "The magnitude of a coefficient represents the strength of the relationship. Larger magnitudes indicate stronger\n",
    "impacts on the response variable.\n",
    "\n",
    "Coefficient Equal to Zero:\n",
    "\n",
    "One of the key features of Lasso Regression is that it can drive coefficients to exactly zero.\n",
    "A coefficient of zero indicates that the corresponding predictor has been excluded from the model's prediction,\n",
    "effectively performing feature selection.\n",
    "A coefficient being exactly zero means that the predictor is not contributing to the model's predictions and can\n",
    "be omitted from consideration.\n",
    "\n",
    "Relative Importance:\n",
    "\n",
    "The coefficients that are non-zero contribute to the model's predictions.\n",
    "The relative importance of features is indicated by the magnitudes of the non-zero coefficients. Larger magnitudes\n",
    "suggest stronger predictive power.\n",
    "\n",
    "Comparing Coefficients:\n",
    "\n",
    "You can compare the magnitudes of non-zero coefficients to understand the relative impact of different predictors \n",
    "on the response variable.\n",
    "A predictor with a larger non-zero coefficient has a greater influence on the model's predictions.\n",
    "\n",
    "Zero vs. Non-Zero Coefficients:\n",
    "\n",
    "When interpreting Lasso coefficients, consider both zero and non-zero coefficients together.\n",
    "Zero coefficients indicate exclusion, while non-zero coefficients indicate active predictors.\n",
    "\n",
    "Interpretation Challenges:\n",
    "\n",
    "Due to the sparsity introduced by Lasso, the coefficients can be challenging to directly compare across different \n",
    "models or datasets with varying feature sets.\n",
    "The interpretation becomes more straightforward when a coefficient is non-zero, as it directly contributes to the \n",
    "model's predictions.\n",
    "\n",
    "Feature Selection and Trade-offs:\n",
    "\n",
    "Keep in mind that while Lasso's feature selection behavior is advantageous, you are trading off interpretability \n",
    "for the sake of a simpler model.\n",
    "Carefully assess the business or research context to ensure that the selected features align with the problem and \n",
    "provide actionable insights. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857ad91e-d57a-45a0-9a2b-3b29d42a4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" In Lasso Regression, there are primarily two tuning parameters that can be adjusted to control the behavior \n",
    "of the model: the regularization parameter (lambda) and the type of normalization applied to the predictor \n",
    "variables. These tuning parameters play a crucial role in determining the model's performance and behavior.\n",
    "Let's explore how each parameter affects the model:\n",
    "\n",
    "Regularization Parameter (Lambda):\n",
    "\n",
    "Lambda controls the strength of the regularization term added to the cost function. It balances the trade-off\n",
    "between fitting the data and preventing overfitting.\n",
    "Smaller values of lambda result in weaker regularization, allowing the model to closely fit the training data. \n",
    "This can lead to overfitting.\n",
    "Larger values of lambda increase the strength of the regularization, causing coefficients to be shrunken towards\n",
    "zero and resulting in simpler models with potential bias.\n",
    "The optimal value of lambda depends on the specific dataset and the desired balance between model complexity and\n",
    "fit. Cross-validation techniques are commonly used to find the optimal lambda.\n",
    "\n",
    "Normalization Type (Optional):\n",
    "\n",
    "Lasso Regression can be applied with or without normalization of the predictor variables. The two common types of\n",
    "normalization are:\n",
    "Standardization (Z-score normalization): This scales the predictor variables to have a mean of zero and a standard\n",
    "deviation of one. It's useful when predictor variables have different scales.\n",
    "Min-Max Scaling (Normalization): This scales the predictor variables to a specific range, often between 0 and 1. \n",
    "It's suitable when you want to maintain the original data distribution.\n",
    "Normalization can affect the impact of lambda on different predictors. It can also affect the interpretation of \n",
    "the coefficients.\n",
    "The effects of these tuning parameters on the model's performance can be summarized as follows:\n",
    "\n",
    "Lambda:\n",
    "\n",
    "Smaller lambda values lead to less regularization, potentially resulting in overfitting and high variance.\n",
    "Larger lambda values lead to stronger regularization, potentially resulting in underfitting and high bias.\n",
    "The optimal lambda value is typically determined through cross-validation to find the right balance between bias\n",
    "and variance.\n",
    "\n",
    "Normalization:\n",
    "\n",
    "Standardization is generally preferred in Lasso Regression because it ensures that all predictors are on a similar\n",
    "scale, allowing lambda to affect all predictors more uniformly.\n",
    "Min-Max Scaling can lead to unequal impacts of lambda on different predictors, as the scaling varies across \n",
    "features.\n",
    "The choice between normalization methods depends on the characteristics of the data and the goals of the \n",
    "analysis. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7b240-8530-4572-b606-a2b532af12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Lasso Regression is primarily designed for linear regression problems, meaning it's intended to model linear\n",
    "relationships between predictors and the response variable. However, with some modifications and extensions, it \n",
    "is possible to adapt Lasso Regression for non-linear regression problems. Here's how you can use Lasso for non\n",
    "-linear regression:\n",
    "\n",
    "Feature Transformation:\n",
    "\n",
    "One way to apply Lasso to non-linear regression is by transforming the original features into a higher-dimensional\n",
    "space using non-linear functions.\n",
    "You can create new features by applying mathematical functions like exponentials, logarithms, polynomials, or \n",
    "trigonometric functions to the original features.\n",
    "After transforming the features, you can use Lasso Regression to model the relationships in the transformed space.\n",
    "\n",
    "Kernel Regression:\n",
    "\n",
    "Kernel methods can be used to implicitly transform the features into a higher-dimensional space without explicitly\n",
    "computing the transformed features.\n",
    "Kernel Regression, a type of non-linear regression, can be combined with Lasso-like regularization to achieve\n",
    "non-linear regression while performing feature selection.\n",
    "\n",
    "Interaction Terms:\n",
    "\n",
    "Introducing interaction terms between predictors can capture non-linear relationships in Lasso Regression.\n",
    "Interaction terms are products of two or more predictors, and they can help model complex interactions between \n",
    "variables.\n",
    "\n",
    "Piecewise Linear Approximation:\n",
    "\n",
    "For piecewise-linear relationships, you can divide the data into segments and apply Lasso Regression separately\n",
    "to each segment.\n",
    "This approach approximates a non-linear function with linear segments, allowing Lasso to capture non-linear \n",
    "behavior.\n",
    "\n",
    "Polynomial Regression:\n",
    "\n",
    "Polynomial Regression is a form of linear regression where polynomial terms of the predictors are included in \n",
    "the model.\n",
    "You can use Lasso Regression with polynomial terms to capture non-linear relationships.\n",
    "\n",
    "Regularization on Non-Linear Components:\n",
    "\n",
    "If you use non-linear transformations on predictors, you can still apply Lasso-like regularization on the \n",
    "transformed components to achieve feature selection and model simplicity. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cedfba9-77f1-4f20-ac02-0f95b407b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q6. What is the difference between Ridge Regression and Lasso Regression? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Ridge Regression and Lasso Regression are both regularization techniques used to improve linear regression \n",
    "models by adding penalty terms to the cost function. These penalties help prevent overfitting and improve the \n",
    "model's generalization performance. While both methods share similarities, they differ in terms of the type of\n",
    "penalty they use and how they affect the model's behavior. Here's a comparison of Ridge Regression and Lasso \n",
    "Regression:\n",
    "\n",
    "Penalty Types:\n",
    "\n",
    "Ridge Regression: Ridge Regression adds a penalty term based on the sum of the squared coefficients (L2 \n",
    "regularization). The penalty term is proportional to the square of the coefficient values.\n",
    "Lasso Regression: Lasso Regression adds a penalty term based on the sum of the absolute values of the coefficients\n",
    "(L1 regularization). The penalty term is proportional to the absolute value of the coefficient values.\n",
    "\n",
    "Coefficient Shrinkage:\n",
    "\n",
    "Ridge Regression: Ridge Regression shrinks the coefficients towards zero by reducing their magnitudes, but it does\n",
    "not drive coefficients exactly to zero. It mitigates multicollinearity by equally shrinking correlated coefficients.\n",
    "Lasso Regression: Lasso Regression can drive some coefficients exactly to zero, effectively performing feature \n",
    "selection. It can eliminate irrelevant predictors and select a subset of the most important ones.\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "Ridge Regression: Ridge Regression does not perform explicit feature selection. It retains all predictors in the \n",
    "model, although they might have smaller magnitudes due to regularization.\n",
    "Lasso Regression: Lasso Regression performs feature selection by driving some coefficients to zero. It selects a \n",
    "subset of the most relevant predictors and eliminates others.\n",
    "\n",
    "Multicollinearity Handling:\n",
    "\n",
    "Ridge Regression: Ridge Regression is effective at handling multicollinearity by shrinking correlated coefficients\n",
    "together. It helps stabilize the model when predictors are highly correlated.\n",
    "Lasso Regression: Lasso Regression's feature selection behavior can be particularly advantageous in cases of \n",
    "multicollinearity, as it helps to identify and retain the most relevant predictors.\n",
    "\n",
    "Model Complexity:\n",
    "\n",
    "Ridge Regression: Ridge Regression reduces the magnitudes of coefficients but rarely sets them exactly to zero.\n",
    "It achieves a balance between fitting the data and preventing overfitting.\n",
    "Lasso Regression: Lasso Regression can significantly reduce the model's complexity by driving some coefficients\n",
    "to zero. It can lead to simpler models with fewer predictors.\n",
    "\n",
    "Bias-Variance Trade-off:\n",
    "\n",
    "Both Ridge and Lasso Regression provide a bias-variance trade-off. Increasing the regularization parameter (lambda)\n",
    "increases bias and reduces variance, helping to control overfitting.\n",
    "\n",
    "Lambda Tuning:\n",
    "\n",
    "Both methods require tuning the regularization parameter (lambda) to strike the right balance between regularization\n",
    "and model fit. Cross-validation is commonly used to find the optimal lambda. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7bf7dc-e134-4a0a-9603-b2c6ecc3c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Yes, Lasso Regression can handle multicollinearity in the input features, although its approach to addressing\n",
    "multicollinearity is different from that of Ridge Regression. While Ridge Regression reduces the impact of \n",
    "multicollinearity by shrinking correlated coefficients together without driving any to zero, Lasso Regression's\n",
    "feature selection behavior allows it to effectively address multicollinearity by driving some coefficients to \n",
    "exactly zero.\n",
    "\n",
    "Here's how Lasso Regression handles multicollinearity:\n",
    "\n",
    "Coefficient Shrinkage:\n",
    "\n",
    "Lasso Regression applies a penalty term based on the sum of the absolute values of coefficients (L1 regularization).\n",
    "This penalty causes the magnitudes of coefficients to be shrunk towards zero, and as the penalty strength (lambda) \n",
    "increases, the coefficients are shrunk more aggressively.\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "One of the key advantages of Lasso Regression is its ability to drive some coefficients to exactly zero, effectively\n",
    "performing automatic feature selection.\n",
    "When faced with multicollinearity, Lasso tends to preferentially select one predictor among the correlated set and \n",
    "drive the coefficients of the others to zero.\n",
    "\n",
    "Selecting Relevant Predictors:\n",
    "\n",
    "In the presence of multicollinearity, Lasso Regression's feature selection behavior can help identify the most \n",
    "relevant predictors.\n",
    "It chooses predictors that contribute the most to the model's performance while eliminating those that are less\n",
    "influential.\n",
    "\n",
    "Trade-off between Predictors:\n",
    "\n",
    "Lasso Regression balances the trade-off between correlated predictors by choosing one and zeroing others.\n",
    "The choice of which predictor to keep and which to eliminate depends on their individual predictive power.\n",
    "\n",
    "Enhancing Model Stability:\n",
    "\n",
    "By eliminating less relevant predictors, Lasso improves the model's stability and reduces the risk of overfitting\n",
    "due to multicollinearity.\n",
    "\n",
    "Deterministic Selection:\n",
    "\n",
    "Lasso's deterministic feature selection behavior means that, in cases of perfectly correlated predictors, it may \n",
    "choose one predictor and ignore the others, based on the optimization process.\n",
    "\n",
    "\n",
    "It's important to note that while Lasso Regression's feature selection behavior is advantageous in handling \n",
    "multicollinearity, it may also lead to the exclusion of predictors that, while correlated, are still relevant to\n",
    "the problem. In scenarios where multicollinearity is a concern but you want to retain all predictors, Ridge \n",
    "Regression or other techniques that prioritize coefficient shrinkage without feature elimination might be more\n",
    "appropriate. The choice between Ridge and Lasso depends on your specific goals, the characteristics of your data,\n",
    "and the level of multicollinearity you're dealing with. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732c909-05b0-46b3-8908-c0189a9dbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Choosing the optimal value of the regularization parameter (lambda) in Lasso Regression is a critical step to \n",
    "achieve a balance between model complexity and predictive performance. The goal is to find the value of lambda that\n",
    "prevents overfitting while still allowing the model to capture the underlying relationships in the data. \n",
    "Cross-validation is commonly used to select the optimal lambda value. Here's a step-by-step process:\n",
    "\n",
    "Split Data:\n",
    "\n",
    "Divide your dataset into two parts: a training set and a validation set (or multiple folds if using k-fold \n",
    "cross-validation).\n",
    "\n",
    "Create a Lambda Range:\n",
    "\n",
    "Define a range of possible lambda values to explore. You can start with a wide range and then narrow it down \n",
    "based on the results.\n",
    "\n",
    "Model Training:\n",
    "\n",
    "For each lambda value in the range, perform the following steps:\n",
    "\n",
    "Standardize Features:\n",
    "\n",
    "Optionally, standardize the predictor variables (Z-score normalization) to ensure fair comparison across features.\n",
    "This is important because Lasso is sensitive to the scale of features.\n",
    "\n",
    "Fit Lasso Regression:\n",
    "\n",
    "Fit a Lasso Regression model on the training data using the current lambda value. The model will automatically \n",
    "perform feature selection and shrinkage.\n",
    "\n",
    "Predict and Evaluate:\n",
    "\n",
    "Use the trained model to predict the outcomes on the validation set. Calculate the evaluation metric of choice \n",
    "(e.g., mean squared error, mean absolute error, R-squared) to measure the model's performance.\n",
    "\n",
    "Choose Optimal Lambda:\n",
    "\n",
    "Select the lambda value that gives the best performance on the validation set. This could be the lambda that \n",
    "minimizes the chosen evaluation metric.\n",
    "\n",
    "Final Model:\n",
    "\n",
    "After choosing the optimal lambda, train the final Lasso Regression model on the entire training dataset using \n",
    "that lambda.\n",
    "\n",
    "Test Set Evaluation:\n",
    "\n",
    "Evaluate the final Lasso Regression model on a separate test set that wasn't used during training or validation.\n",
    "This provides an unbiased estimate of the model's performance on new, unseen data.\n",
    "\n",
    "Additional Considerations:\n",
    "\n",
    "If your dataset is small, consider using k-fold cross-validation to ensure robustness in the lambda selection \n",
    "process.\n",
    "Some libraries or frameworks provide built-in functions for automatically performing cross-validation to find the\n",
    "optimal lambda value.\n",
    "\n",
    "\n",
    "It's important to note that the optimal lambda value may vary depending on the specific characteristics of your \n",
    "dataset. Cross-validation helps you determine the most suitable lambda for your problem and prevent overfitting.\n",
    "Remember that the ultimate goal is to strike a balance between model complexity and predictive performance, so\n",
    "it's essential to consider both during the lambda selection process. \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
